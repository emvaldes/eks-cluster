#!/bin/bash
## Version: 0.1.0

script_name=$(basename ${0});

function how2create_cluster () {
    ## tracking_process ${FUNCNAME} "${@}";
    display_message "$ eks-cluster \
  --create-cluster=kubernetes.devops \
  --access-pubkey=~/.ssh/kubernetes.pub \
  --verbose \
  --debug \
;

Function [deploy_kubernetes]:
[ℹ]  eksctl version 0.11.1
[ℹ]  using region us-east-1
[ℹ]  subnets for us-east-1a - public:192.168.0.0/19 private:192.168.96.0/19
[ℹ]  subnets for us-east-1b - public:192.168.32.0/19 private:192.168.128.0/19
[ℹ]  subnets for us-east-1c - public:192.168.64.0/19 private:192.168.160.0/19
[ℹ]  using SSH public key \"/Users/prototype/.ssh/kubernetes.pub\" as \"eksctl-prototype-nodegroup-devops-b5:89:0b:9d:9a:58:53:b0:06:df:11:11:87:60:74:c7\"
[ℹ]  using Kubernetes version 1.14
[ℹ]  creating EKS cluster \"prototype\" in \"us-east-1\" region with managed nodes
[ℹ]  will create 2 separate CloudFormation stacks for cluster itself and the initial managed nodegroup
[ℹ]  if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=us-east-1 --cluster=prototype'
[ℹ]  CloudWatch logging will not be enabled for cluster \"prototype\" in \"us-east-1\"
[ℹ]  you can enable it with 'eksctl utils update-cluster-logging --region=us-east-1 --cluster=prototype'
[ℹ]  Kubernetes API endpoint access will use default of {publicAccess=true, privateAccess=false} for cluster \"prototype\" in \"us-east-1\"
[ℹ]  2 sequential tasks: { create cluster control plane \"prototype\", create managed nodegroup \"devops\" }
[ℹ]  building cluster stack \"eksctl-prototype-cluster\"
[ℹ]  deploying stack \"eksctl-prototype-cluster\"
[ℹ]  building managed nodegroup stack \"eksctl-prototype-nodegroup-devops\"
[ℹ]  deploying stack \"eksctl-prototype-nodegroup-devops\"
[✔]  all EKS cluster resources for \"prototype\" have been created
[✔]  saved kubeconfig as \"/Users/prototype/.kube/eksctl/clusters/prototype\"
[ℹ]  nodegroup \"devops\" has 2 node(s)
[ℹ]  node \"ip-192-168-52-90.ec2.internal\" is ready
[ℹ]  node \"ip-192-168-66-209.ec2.internal\" is ready
[ℹ]  waiting for at least 2 node(s) to become ready in \"devops\"
[ℹ]  nodegroup \"devops\" has 2 node(s)
[ℹ]  node \"ip-192-168-52-90.ec2.internal\" is ready
[ℹ]  node \"ip-192-168-66-209.ec2.internal\" is ready
[ℹ]  kubectl command should work with \"/Users/prototype/.kube/eksctl/clusters/prototype\", try 'kubectl --kubeconfig=/Users/prototype/.kube/eksctl/clusters/prototype get nodes'
[✔]  EKS cluster \"prototype\" in \"us-east-1\" region is ready
[ℹ]  eksctl version 0.11.1
[ℹ]  using region us-east-1
[✔]  saved kubeconfig as \"/Users/prototype/.kube/eksctl/clusters/prototype\"
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: DATA+OMITTED
    server: https://31D0097B30C6300A1AD47CD740DB4E4E.yl4.us-east-1.eks.amazonaws.com
  name: prototype.us-east-1.eksctl.io
contexts:
- context:
    cluster: prototype.us-east-1.eksctl.io
    user: kubernetes@prototype.us-east-1.eksctl.io
  name: kubernetes@prototype.us-east-1.eksctl.io
current-context: kubernetes@prototype.us-east-1.eksctl.io
kind: Config
preferences: {}
users:
- name: kubernetes@prototype.us-east-1.eksctl.io
  user:
    exec:
      apiVersion: client.authentication.k8s.io/v1alpha1
      args:
      - token
      - -i
      - prototype
      command: aws-iam-authenticator
      env:
      - name: AWS_PROFILE
        value: kubernetes

Continue [Y/n]?: y

Function [cluster_token]:
{
    \"apiVersion\": \"client.authentication.k8s.io/v1alpha1\",
    \"kind\": \"ExecCredential\",
    \"spec\": {},
    \"status\": {
        \"expirationTimestamp\": \"2019-12-23T08:57:47Z\",
        \"token\": \"k8s-aws-v1.aHR0cHM6Ly9zdHMuYW1hem9uYXdzLmNvbS8_QWN0aW9uPUdldENhbGxlcklkZW50aXR5JlZlcnNpb249MjAxMS0wNi0xNSZYLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFaWlAyNjU0MkJLQzI3TDRZJTJGMjAxOTEyMjMlMkZ1cy1lYXN0LTElMkZzdHMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDE5MTIyM1QwODQzNDdaJlgtQW16LUV4cGlyZXM9MCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QlM0J4LWs4cy1hd3MtaWQmWC1BbXotU2lnbmF0dXJlPWM4NjI1NjU2MmM5YzRhYzk5YjJhMzBmMzFiMGQwMzExYzFmYzkyNjY5ZDYxZGIwZDVlODQ3ODAzYjMwZGJlZWQ\"
    }
}

Continue [Y/n]?: y

Function [configure_logging]:
[ℹ]  eksctl version 0.11.1
[ℹ]  using region us-east-1
[ℹ]  will update CloudWatch logging for cluster \"prototype\" in \"us-east-1\" (enable types: api, audit, authenticator, controllerManager, scheduler & no types to disable)
[✔]  configured CloudWatch logging for cluster \"prototype\" in \"us-east-1\" (enabled types: api, audit, authenticator, controllerManager, scheduler & no types disabled)

Continue [Y/n]?: y

Function [metricsserver_deploy]:
clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created
clusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created
rolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created
apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created
serviceaccount/metrics-server created
deployment.apps/metrics-server created
service/metrics-server created
clusterrole.rbac.authorization.k8s.io/system:metrics-server created
clusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created

Continue [Y/n]?: y

Function [describe_stacks]:
[ℹ]  eksctl version 0.11.1
[ℹ]  using region us-east-1
[ℹ]  stack/eksctl-prototype-nodegroup-devops = {
  Capabilities: [\"CAPABILITY_IAM\"],
  CreationTime: 2019-12-23 08:41:16.73 +0000 UTC,
  Description: \"EKS Managed Nodes (SSH access: true) [created by eksctl]\",
  DisableRollback: false,
  DriftInformation: {
    StackDriftStatus: \"NOT_CHECKED\"
  },
  EnableTerminationProtection: false,
  RollbackConfiguration: {

  },
  StackId: \"arn:aws:cloudformation:us-east-1:738054984624:stack/eksctl-prototype-nodegroup-devops/fbe8cbc0-255f-11ea-ae41-0afe492705d7\",
  StackName: \"eksctl-prototype-nodegroup-devops\",
  StackStatus: \"CREATE_COMPLETE\",
  Tags: [
    {
      Key: \"alpha.eksctl.io/cluster-name\",
      Value: \"prototype\"
    },
    {
      Key: \"Owner\",
      Value: \"SRE Team\"
    },
    {
      Key: \"alpha.eksctl.io/nodegroup-name\",
      Value: \"devops\"
    },
    {
      Key: \"eksctl.cluster.k8s.io/v1alpha1/cluster-name\",
      Value: \"prototype\"
    },
    {
      Key: \"Team\",
      Value: \"DevOps Team\"
    },
    {
      Key: \"alpha.eksctl.io/nodegroup-type\",
      Value: \"managed\"
    },
    {
      Key: \"Creator\",
      Value: \"Eduardo Valdes\"
    }
  ]
}
[ℹ]  stack/eksctl-prototype-cluster = {
  Capabilities: [\"CAPABILITY_IAM\"],
  CreationTime: 2019-12-23 08:28:34.88 +0000 UTC,
  Description: \"EKS cluster (dedicated VPC: true, dedicated IAM: true) [created and managed by eksctl]\",
  DisableRollback: false,
  DriftInformation: {
    StackDriftStatus: \"NOT_CHECKED\"
  },
  EnableTerminationProtection: false,
  Outputs: [
    {
      ExportName: \"eksctl-prototype-cluster::SubnetsPrivate\",
      OutputKey: \"SubnetsPrivate\",
      OutputValue: \"subnet-06f80d112a35bcbc5,subnet-088cf0f3c3ca8d8a3,subnet-03804f05568eb39ba\"
    },
    {
      ExportName: \"eksctl-prototype-cluster::SubnetsPublic\",
      OutputKey: \"SubnetsPublic\",
      OutputValue: \"subnet-0f5f396fc169fa3cc,subnet-09a7c44bb0d763ea1,subnet-0f2ee3e2b65f9660f\"
    },
    {
      ExportName: \"eksctl-prototype-cluster::ServiceRoleARN\",
      OutputKey: \"ServiceRoleARN\",
      OutputValue: \"arn:aws:iam::738054984624:role/eksctl-prototype-cluster-ServiceRole-1NTUNUAY288AO\"
    },
    {
      ExportName: \"eksctl-prototype-cluster::VPC\",
      OutputKey: \"VPC\",
      OutputValue: \"vpc-094eb6f95cb478207\"
    },
    {
      OutputKey: \"ClusterStackName\",
      OutputValue: \"eksctl-prototype-cluster\"
    },
    {
      OutputKey: \"CertificateAuthorityData\",
      OutputValue: \"LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRFNU1USXlNekE0TXpneU9Wb1hEVEk1TVRJeU1EQTRNemd5T1Zvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBS21RCmJFRmZkTEdTSUU5YTgzY3ZIMFBqblJOZEo3aHN2VDV2UlpLcFlOZlg3TWdiZ0hVOG11WXdJYnZVQjJDSXZNK3kKN2M2QVozeUsrSTd5WjIxck5kSlBsaFV5RThZd09RdGJxOGNDdHl2RklFUnRxRXVrbmo2MW1ET01LTXFjeDdpQgpySTdHWVFrbmtYME83VUdhQWRpOExIaXBCTXdpTHI5bEhHQXhQSGx6V3pJcTdEUXNBc0VnaW9CTHhNVUtOaFRwCmljeFFIOVU2MXdtZGFIeXJ3WFRubm4vWGRBdUtmWVRmOWZzczlXa2ZDTTdCdmZzNjUxaUh5WVFsaDJJNjJCcTAKWWFDNUt1ODFkam4rZXVZak1TTFg1QlJrZTdDMFBqeHgzT0huek52R2lzVGNxcHNjZktuWVRRWStpT0IzNDh6SwpKYzRnYkNNWFl1djhUalB1bTVjQ0F3RUFBYU1qTUNFd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFIZ3RsSk9zTWkraWZST1RMWERrLzhwbksyOHIKNGdvWFpCTXNoaVpKYXhWMWo1WURTSUc5dzZlNUUyZlQ3d05Ud0ZLWWk2MXNodERIK1NLaGRRaFBtN1FaLy9EOAphaGRORnNpTkdYVHhMVzhPSlNXa1graWZXWStKS3AzWWVlWkN1bDFYd3lkc0xsMGJSYldaNVBEdk9VQW1jNng3CmcyZEMyekxBaklRdU1KbHdjWGFEYmF6UzRra1VSVkpWZFNpUW5tOW5nZmttWDZrSmtJeDJENmZjV0FSeUFZVGgKVGJsWFFoMGY5L2YweDdIUHNvOFBWeDJXV1JRbnZoanlnVVlOK3BUNEduTm9DZkliVHBVS2pOQjJsMkpjNElZawpUVmhKSHlSaVNzSWZLL1J5T0F0RWxvckRqU0N4YXFmV2tVU0pYZXFjM2IvVkJIa1RVeVhvRElwTEVYTT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=\"
    },
    {
      ExportName: \"eksctl-prototype-cluster::SecurityGroup\",
      OutputKey: \"SecurityGroup\",
      OutputValue: \"sg-054594ae12482ff56\"
    },
    {
      OutputKey: \"FeatureNATMode\",
      OutputValue: \"Single\"
    },
    {
      ExportName: \"eksctl-prototype-cluster::Endpoint\",
      OutputKey: \"Endpoint\",
      OutputValue: \"https://31D0097B30C6300A1AD47CD740DB4E4E.yl4.us-east-1.eks.amazonaws.com\"
    },
    {
      ExportName: \"eksctl-prototype-cluster::SharedNodeSecurityGroup\",
      OutputKey: \"SharedNodeSecurityGroup\",
      OutputValue: \"sg-0992e20edb053545d\"
    },
    {
      ExportName: \"eksctl-prototype-cluster::ClusterSecurityGroupId\",
      OutputKey: \"ClusterSecurityGroupId\",
      OutputValue: \"sg-0802d870efe4f66a2\"
    },
    {
      ExportName: \"eksctl-prototype-cluster::ARN\",
      OutputKey: \"ARN\",
      OutputValue: \"arn:aws:eks:us-east-1:738054984624:cluster/prototype\"
    },
    {
      ExportName: \"eksctl-prototype-cluster::FargatePodExecutionRoleARN\",
      OutputKey: \"FargatePodExecutionRoleARN\",
      OutputValue: \"arn:aws:iam::738054984624:role/eksctl-prototype-cluster-FargatePodExecutionRole-ZUY6UD2QGJSV\"
    }
  ],
  RollbackConfiguration: {

  },
  StackId: \"arn:aws:cloudformation:us-east-1:738054984624:stack/eksctl-prototype-cluster/35ae2460-255e-11ea-b30f-0a4f04019892\",
  StackName: \"eksctl-prototype-cluster\",
  StackStatus: \"CREATE_COMPLETE\",
  Tags: [
    {
      Key: \"alpha.eksctl.io/cluster-name\",
      Value: \"prototype\"
    },
    {
      Key: \"Owner\",
      Value: \"SRE Team\"
    },
    {
      Key: \"eksctl.cluster.k8s.io/v1alpha1/cluster-name\",
      Value: \"prototype\"
    },
    {
      Key: \"Team\",
      Value: \"DevOps Team\"
    },
    {
      Key: \"Creator\",
      Value: \"Eduardo Valdes\"
    }
  ]
}

Continue [Y/n]?: y

Function [display_nodes]:

NAME                             STATUS   ROLES    AGE    VERSION              INTERNAL-IP      EXTERNAL-IP    OS-IMAGE         KERNEL-VERSION                  CONTAINER-RUNTIME
ip-192-168-52-90.ec2.internal    Ready    <none>   106s   v1.14.7-eks-1861c5   192.168.52.90    3.82.219.54    Amazon Linux 2   4.14.146-119.123.amzn2.x86_64   docker://18.6.1
ip-192-168-66-209.ec2.internal   Ready    <none>   2m1s   v1.14.7-eks-1861c5   192.168.66.209   3.87.238.174   Amazon Linux 2   4.14.146-119.123.amzn2.x86_64   docker://18.6.1

Continue [Y/n]?: y

Function [create_autoscaler]:
serviceaccount/cluster-autoscaler created
clusterrole.rbac.authorization.k8s.io/cluster-autoscaler created
role.rbac.authorization.k8s.io/cluster-autoscaler created
clusterrolebinding.rbac.authorization.k8s.io/cluster-autoscaler created
rolebinding.rbac.authorization.k8s.io/cluster-autoscaler created
deployment.apps/cluster-autoscaler created

Continue [Y/n]?: y

Function [autoscaler_group]:
Auto-Scaling Group Name: eks-9cb798f5-9fda-4fbf-09e1-e1773d132961

Continue [Y/n]?: y

Function [disable_eviction]:
deployment.apps/cluster-autoscaler annotated

Continue [Y/n]?: y

Function [autoscaler_version]:
deployment.apps/cluster-autoscaler image updated

Continue [Y/n]?: y

Function [autoscaler_update]:

Modify these lines:
- --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/prototype

Append these lines:
- --balance-similar-node-groups
- --skip-nodes-with-system-pods=false
deployment.apps/cluster-autoscaler edited

Continue [Y/n]?: y

Function [inspect_autoscaler]:

Continue [Y/n]?: y

Function [configmap_awsauth]:
/tmp/devops/prototype/aws-auth-cm.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: aws-auth
  namespace: kube-system
data:
  mapRoles: |
    - rolearn: arn:aws:iam::738054984624:role/eksctl-prototype-nodegroup-devops-NodeInstanceRole-EAECB0XT8T2L
      username: system:node:{{EC2PrivateDNSName}}
      groups:
        - system:bootstrappers
        - system:nodes
Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply
configmap/aws-auth configured
Name:         aws-auth
Namespace:    kube-system
Labels:       <none>
Annotations:  kubectl.kubernetes.io/last-applied-configuration:
                {\"apiVersion\":\"v1\",\"data\":{\"mapRoles\":\"- rolearn: arn:aws:iam::738054984624:role/eksctl-prototype-nodegroup-devops-NodeInstanceRole-EAECB0XT...

Data
====
mapRoles:
----
- rolearn: arn:aws:iam::738054984624:role/eksctl-prototype-nodegroup-devops-NodeInstanceRole-EAECB0XT8T2L
  username: system:node:{{EC2PrivateDNSName}}
  groups:
    - system:bootstrappers
    - system:nodes

Events:  <none>

Continue [Y/n]?: y

Function [metricshelper_deploy]:
clusterrole.rbac.authorization.k8s.io/cni-metrics-helper created
serviceaccount/cni-metrics-helper created
clusterrolebinding.rbac.authorization.k8s.io/cni-metrics-helper created
deployment.extensions/cni-metrics-helper created

Continue [Y/n]?: y

Function [metricshelper_policy]:
{
  \"Version\": \"2012-10-17\",
  \"Statement\": [
    {
      \"Effect\": \"Allow\",
      \"Action\": \"cloudwatch:PutMetricData\",
      \"Resource\": \"*\"
    }
  ]
}

An error occurred (EntityAlreadyExists) when calling the CreatePolicy operation: A policy called CNIMetricsHelperPolicy--prototype-devops already exists. Duplicate names are not allowed.

Continue [Y/n]?: y

Function [metricshelper_rolepolicies]:

Continue [Y/n]?: y

Function [instance_roleupdate]:
{
    \"IamInstanceProfileAssociation\": {
        \"AssociationId\": \"iip-assoc-010afe541671c251e\",
        \"InstanceId\": \"i-034c200337e766387\",
        \"IamInstanceProfile\": {
            \"Arn\": \"arn:aws:iam::738054984624:instance-profile/eks-9cb798f5-9fda-4fbf-09e1-e1773d132961\",
            \"Id\": \"AIPAZZP26542MDVOITYAD\"
        },
        \"State\": \"associated\"
    }
}
{
    \"IamInstanceProfileAssociation\": {
        \"AssociationId\": \"iip-assoc-044c76b85b2cf39d0\",
        \"InstanceId\": \"i-074e0f0d3ab1e6dab\",
        \"IamInstanceProfile\": {
            \"Arn\": \"arn:aws:iam::738054984624:instance-profile/eks-9cb798f5-9fda-4fbf-09e1-e1773d132961\",
            \"Id\": \"AIPAZZP26542MDVOITYAD\"
        },
        \"State\": \"associated\"
    }
}

Continue [Y/n]?: y

Function [cloudwatch_namespace]:
namespace/amazon-cloudwatch created

Continue [Y/n]?: y

Function [cwagent_fluentd]:
namespace/amazon-cloudwatch unchanged
serviceaccount/cloudwatch-agent created
clusterrole.rbac.authorization.k8s.io/cloudwatch-agent-role created
clusterrolebinding.rbac.authorization.k8s.io/cloudwatch-agent-role-binding created
configmap/cwagentconfig created
daemonset.apps/cloudwatch-agent created
configmap/cluster-info created
serviceaccount/fluentd created
clusterrole.rbac.authorization.k8s.io/fluentd-role created
clusterrolebinding.rbac.authorization.k8s.io/fluentd-role-binding created
configmap/fluentd-config created
daemonset.apps/fluentd-cloudwatch created

Continue [Y/n]?: y

Function [display_pods]: amazon-cloudwatch
NAME                       READY   STATUS              RESTARTS   AGE   IP       NODE                             NOMINATED NODE   READINESS GATES
cloudwatch-agent-t6nxd     0/1     ContainerCreating   0          2s    <none>   ip-192-168-52-90.ec2.internal    <none>           <none>
fluentd-cloudwatch-kx4k4   0/1     Init:0/2            0          1s    <none>   ip-192-168-52-90.ec2.internal    <none>           <none>
cloudwatch-agent-k2pqs     0/1     ContainerCreating   0          2s    <none>   ip-192-168-66-209.ec2.internal   <none>           <none>
fluentd-cloudwatch-p5t8x   0/1     Init:0/2            0          1s    <none>   ip-192-168-66-209.ec2.internal   <none>           <none>

Continue [Y/n]?: y

Function [cwagent_serviceaccount]:
serviceaccount/cloudwatch-agent unchanged
clusterrole.rbac.authorization.k8s.io/cloudwatch-agent-role unchanged
clusterrolebinding.rbac.authorization.k8s.io/cloudwatch-agent-role-binding unchanged

Continue [Y/n]?: y

Function [configmap_cwagent]:
/tmp/devops/prototype/cwagent-configmap.yaml
configmap/cwagentconfig configured

Continue [Y/n]?: y

Function [cwagent_daemonset]:
daemonset.apps/cloudwatch-agent unchanged

Continue [Y/n]?: y

Function [cluster_podname]: amazon-cloudwatch

Continue [Y/n]?: y

Describing Pod: cloudwatch-agent-k2pqs

Function [describe_podname]: cloudwatch-agent-k2pqs
Name:           cloudwatch-agent-k2pqs
Namespace:      amazon-cloudwatch
Priority:       0
Node:           ip-192-168-66-209.ec2.internal/192.168.66.209
Start Time:     Mon, 23 Dec 2019 01:46:12 -0700
Labels:         controller-revision-hash=7cb45c7f8c
                name=cloudwatch-agent
                pod-template-generation=1
Annotations:    kubernetes.io/psp: eks.privileged
Status:         Running
IP:             192.168.65.184
IPs:            <none>
Controlled By:  DaemonSet/cloudwatch-agent
Containers:
  cloudwatch-agent:
    Container ID:   docker://ddd8e75d6f1cbc17f6c7a7eaad9b16fb148883139f1c5a38b072842d1fd171a9
    Image:          amazon/cloudwatch-agent:1.230621.0
    Image ID:       docker-pullable://amazon/cloudwatch-agent@sha256:877106acbc56e747ebe373548c88cd37274f666ca11b5c782211db4c5c7fb64b
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Mon, 23 Dec 2019 01:46:15 -0700
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     200m
      memory:  200Mi
    Requests:
      cpu:     200m
      memory:  200Mi
    Environment:
      HOST_IP:         (v1:status.hostIP)
      HOST_NAME:       (v1:spec.nodeName)
      K8S_NAMESPACE:  amazon-cloudwatch (v1:metadata.namespace)
      CI_VERSION:     k8s/1.0.1
    Mounts:
      /dev/disk from devdisk (ro)
      /etc/cwagentconfig from cwagentconfig (rw)
      /rootfs from rootfs (ro)
      /sys from sys (ro)
      /var/lib/docker from varlibdocker (ro)
      /var/run/docker.sock from dockersock (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from cloudwatch-agent-token-q2l8q (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             True
  ContainersReady   True
  PodScheduled      True
Volumes:
  cwagentconfig:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      cwagentconfig
    Optional:  false
  rootfs:
    Type:          HostPath (bare host directory volume)
    Path:          /
    HostPathType:
  dockersock:
    Type:          HostPath (bare host directory volume)
    Path:          /var/run/docker.sock
    HostPathType:
  varlibdocker:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/docker
    HostPathType:
  sys:
    Type:          HostPath (bare host directory volume)
    Path:          /sys
    HostPathType:
  devdisk:
    Type:          HostPath (bare host directory volume)
    Path:          /dev/disk/
    HostPathType:
  cloudwatch-agent-token-q2l8q:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  cloudwatch-agent-token-q2l8q
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type    Reason     Age   From                                     Message
  ----    ------     ----  ----                                     -------
  Normal  Scheduled  6s    default-scheduler                        Successfully assigned amazon-cloudwatch/cloudwatch-agent-k2pqs to ip-192-168-66-209.ec2.internal
  Normal  Pulling    5s    kubelet, ip-192-168-66-209.ec2.internal  Pulling image \"amazon/cloudwatch-agent:1.230621.0\"
  Normal  Pulled     3s    kubelet, ip-192-168-66-209.ec2.internal  Successfully pulled image \"amazon/cloudwatch-agent:1.230621.0\"
  Normal  Created    3s    kubelet, ip-192-168-66-209.ec2.internal  Created container cloudwatch-agent
  Normal  Started    3s    kubelet, ip-192-168-66-209.ec2.internal  Started container cloudwatch-agent

Continue [Y/n]?: y

Displaying Pod's Log: cloudwatch-agent-k2pqs

2019/12/23 08:46:15 I! I! Detected the instance is EC2
2019/12/23 08:46:15 Reading json config file path: /opt/aws/amazon-cloudwatch-agent/bin/default_linux_config.json ...
/opt/aws/amazon-cloudwatch-agent/bin/default_linux_config.json does not exist or cannot read. Skipping it.
2019/12/23 08:46:15 Reading json config file path: /etc/cwagentconfig/..2019_12_23_08_46_12.683569077/cwagentconfig.json ...
2019/12/23 08:46:15 Find symbolic link /etc/cwagentconfig/..data
2019/12/23 08:46:15 Find symbolic link /etc/cwagentconfig/cwagentconfig.json
2019/12/23 08:46:15 Reading json config file path: /etc/cwagentconfig/cwagentconfig.json ...
Valid Json input schema.
No csm configuration found.
No metric configuration found.
Configuration validation first phase succeeded

2019/12/23 08:46:15 I! Config has been translated into TOML /opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.toml
2019/12/23 08:46:15 I! AmazonCloudWatchAgent Version 1.230621.0.
2019-12-23T08:46:15Z I! Starting AmazonCloudWatchAgent (version 1.230621.0)
2019-12-23T08:46:15Z I! Loaded outputs: cloudwatchlogs
2019-12-23T08:46:15Z I! Loaded inputs: cadvisor k8sapiserver
2019-12-23T08:46:15Z I! Tags enabled:
2019-12-23T08:46:15Z I! Agent Config: Interval:1m0s, Quiet:false, Hostname:\"ip-192-168-66-209.ec2.internal\", Flush Interval:1s
2019-12-23T08:46:15Z I! Cannot get the leader config map: configmaps \"cwagent-clusterleader\" not found, try to create the config map...
2019-12-23T08:46:15Z I! configMap: &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:cwagent-clusterleader,GenerateName:,Namespace:amazon-cloudwatch,SelfLink:/api/v1/namespaces/amazon-cloudwatch/configmaps/cwagent-clusterleader,UID:ae416aad-2560-11ea-83e0-0a20636a20d3,ResourceVersion:1342,Generation:0,CreationTimestamp:2019-12-23 08:46:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},}, err: <nil>
2019-12-23T08:46:15Z I! k8sapiserver OnStartedLeading: ip-192-168-66-209.ec2.internal

Continue [Y/n]?: y

Function [create_namespace]: prometheus
namespace/prometheus created

Continue [Y/n]?: y

Function [service_account]: tiller
apiVersion: v1
kind: ServiceAccount
metadata:
  name: tiller
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: tiller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - kind: ServiceAccount
    name: tiller
    namespace: kube-system

serviceaccount/tiller created
clusterrolebinding.rbac.authorization.k8s.io/tiller created
$HELM_HOME has been configured at /Users/prototype/.helm.

Tiller (the Helm server-side component) has been installed into your Kubernetes Cluster.

Please note: by default, Tiller is deployed with an insecure 'allow unauthenticated users' policy.
To prevent this, run `helm init` with the --tiller-tls-verify flag.
For more information on securing your installation see: https://docs.helm.sh/using_helm/#securing-your-helm-installation

NAME            READY   UP-TO-DATE   AVAILABLE   AGE
tiller-deploy   1/1     1            1           61s

Name:                   tiller-deploy
Namespace:              kube-system
CreationTimestamp:      Mon, 23 Dec 2019 01:46:35 -0700
Labels:                 app=helm
                        name=tiller
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               app=helm,name=tiller
Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:           app=helm
                    name=tiller
  Service Account:  tiller
  Containers:
   tiller:
    Image:       gcr.io/kubernetes-helm/tiller:v2.16.1
    Ports:       44134/TCP, 44135/TCP
    Host Ports:  0/TCP, 0/TCP
    Liveness:    http-get http://:44135/liveness delay=1s timeout=1s period=10s #success=1 #failure=3
    Readiness:   http-get http://:44135/readiness delay=1s timeout=1s period=10s #success=1 #failure=3
    Environment:
      TILLER_NAMESPACE:    kube-system
      TILLER_HISTORY_MAX:  300
    Mounts:                <none>
  Volumes:                 <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   tiller-deploy-54c98f988f (1/1 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  61s   deployment-controller  Scaled up replica set tiller-deploy-54c98f988f to 1

Continue [Y/n]?: y

Function [setup_monitoring]: prometheus
2019/12/23 01:47:38 Warning: Merging destination map for chart 'prometheus-node-exporter'. Overwriting table item 'extraArgs', with non table value: [--collector.filesystem.ignored-mount-points=^/(dev|proc|sys|var/lib/docker/.+)($|/) --collector.filesystem.ignored-fs-types=^(autofs|binfmt_misc|cgroup|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|mqueue|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|sysfs|tracefs)$]
2019/12/23 01:47:38 Warning: Merging destination map for chart 'prometheus-node-exporter'. Overwriting table item 'extraArgs', with non table value: [--collector.filesystem.ignored-mount-points=^/(dev|proc|sys|var/lib/docker/.+)($|/) --collector.filesystem.ignored-fs-types=^(autofs|binfmt_misc|cgroup|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|mqueue|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|sysfs|tracefs)$]
NAME:   prometheus
LAST DEPLOYED: Mon Dec 23 01:47:40 2019
NAMESPACE: prometheus
STATUS: DEPLOYED

RESOURCES:
==> v1/Alertmanager
NAME                                     AGE
prometheus-prometheus-oper-alertmanager  34s

==> v1/ClusterRole
NAME                                       AGE
prometheus-grafana-clusterrole             34s
prometheus-prometheus-oper-alertmanager    34s
prometheus-prometheus-oper-operator        34s
prometheus-prometheus-oper-operator-psp    34s
prometheus-prometheus-oper-prometheus      34s
prometheus-prometheus-oper-prometheus-psp  34s
psp-prometheus-kube-state-metrics          34s

==> v1/ClusterRoleBinding
NAME                                       AGE
prometheus-grafana-clusterrolebinding      34s
prometheus-prometheus-oper-alertmanager    34s
prometheus-prometheus-oper-operator        34s
prometheus-prometheus-oper-operator-psp    34s
prometheus-prometheus-oper-prometheus      34s
prometheus-prometheus-oper-prometheus-psp  34s
psp-prometheus-kube-state-metrics          34s

==> v1/ConfigMap
NAME                                                          AGE
prometheus-grafana                                            34s
prometheus-grafana-config-dashboards                          34s
prometheus-grafana-test                                       34s
prometheus-prometheus-oper-apiserver                          34s
prometheus-prometheus-oper-controller-manager                 34s
prometheus-prometheus-oper-etcd                               34s
prometheus-prometheus-oper-grafana-datasource                 34s
prometheus-prometheus-oper-k8s-resources-cluster              34s
prometheus-prometheus-oper-k8s-resources-namespace            34s
prometheus-prometheus-oper-k8s-resources-pod                  34s
prometheus-prometheus-oper-k8s-resources-workload             34s
prometheus-prometheus-oper-k8s-resources-workloads-namespace  34s
prometheus-prometheus-oper-kubelet                            34s
prometheus-prometheus-oper-node-cluster-rsrc-use              34s
prometheus-prometheus-oper-node-rsrc-use                      34s
prometheus-prometheus-oper-nodes                              34s
prometheus-prometheus-oper-persistentvolumesusage             34s
prometheus-prometheus-oper-pods                               34s
prometheus-prometheus-oper-prometheus                         34s
prometheus-prometheus-oper-prometheus-remote-write            34s
prometheus-prometheus-oper-proxy                              34s
prometheus-prometheus-oper-scheduler                          34s
prometheus-prometheus-oper-statefulset                        34s

==> v1/Deployment
NAME                                 AGE
prometheus-grafana                   34s
prometheus-kube-state-metrics        34s
prometheus-prometheus-oper-operator  34s

==> v1/Pod(related)
NAME                                                  AGE
prometheus-grafana-5c97446694-9bhc7                   34s
prometheus-kube-state-metrics-5ffdf76ddd-f8pgx        34s
prometheus-prometheus-node-exporter-9nbsp             34s
prometheus-prometheus-node-exporter-cnxtz             34s
prometheus-prometheus-oper-operator-6d59dcfb57-2crvb  34s

==> v1/Prometheus
NAME                                   AGE
prometheus-prometheus-oper-prometheus  34s

==> v1/PrometheusRule
NAME                                                             AGE
prometheus-prometheus-oper-alertmanager.rules                    34s
prometheus-prometheus-oper-etcd                                  34s
prometheus-prometheus-oper-general.rules                         34s
prometheus-prometheus-oper-k8s.rules                             34s
prometheus-prometheus-oper-kube-apiserver.rules                  34s
prometheus-prometheus-oper-kube-prometheus-node-recording.rules  34s
prometheus-prometheus-oper-kube-scheduler.rules                  34s
prometheus-prometheus-oper-kubernetes-absent                     34s
prometheus-prometheus-oper-kubernetes-apps                       34s
prometheus-prometheus-oper-kubernetes-resources                  34s
prometheus-prometheus-oper-kubernetes-storage                    34s
prometheus-prometheus-oper-kubernetes-system                     34s
prometheus-prometheus-oper-node-exporter                         34s
prometheus-prometheus-oper-node-exporter.rules                   34s
prometheus-prometheus-oper-node-network                          34s
prometheus-prometheus-oper-node-time                             34s
prometheus-prometheus-oper-node.rules                            34s
prometheus-prometheus-oper-prometheus                            34s
prometheus-prometheus-oper-prometheus-operator                   34s

==> v1/Role
NAME                     AGE
prometheus-grafana-test  34s

==> v1/RoleBinding
NAME                     AGE
prometheus-grafana-test  34s

==> v1/Secret
NAME                                                  AGE
alertmanager-prometheus-prometheus-oper-alertmanager  34s
prometheus-grafana                                    34s

==> v1/Service
NAME                                                AGE
prometheus-grafana                                  34s
prometheus-kube-state-metrics                       34s
prometheus-prometheus-node-exporter                 34s
prometheus-prometheus-oper-alertmanager             34s
prometheus-prometheus-oper-coredns                  34s
prometheus-prometheus-oper-kube-controller-manager  34s
prometheus-prometheus-oper-kube-etcd                34s
prometheus-prometheus-oper-kube-proxy               34s
prometheus-prometheus-oper-kube-scheduler           34s
prometheus-prometheus-oper-operator                 34s
prometheus-prometheus-oper-prometheus               34s

==> v1/ServiceAccount
NAME                                     AGE
prometheus-grafana                       34s
prometheus-grafana-test                  34s
prometheus-kube-state-metrics            34s
prometheus-prometheus-node-exporter      34s
prometheus-prometheus-oper-alertmanager  34s
prometheus-prometheus-oper-operator      34s
prometheus-prometheus-oper-prometheus    34s

==> v1/ServiceMonitor
NAME                                                AGE
prometheus-prometheus-oper-alertmanager             34s
prometheus-prometheus-oper-apiserver                34s
prometheus-prometheus-oper-coredns                  34s
prometheus-prometheus-oper-grafana                  34s
prometheus-prometheus-oper-kube-controller-manager  34s
prometheus-prometheus-oper-kube-etcd                34s
prometheus-prometheus-oper-kube-proxy               34s
prometheus-prometheus-oper-kube-scheduler           34s
prometheus-prometheus-oper-kube-state-metrics       34s
prometheus-prometheus-oper-kubelet                  34s
prometheus-prometheus-oper-node-exporter            34s
prometheus-prometheus-oper-operator                 34s
prometheus-prometheus-oper-prometheus               34s

==> v1beta1/ClusterRole
NAME                                     AGE
prometheus-kube-state-metrics            34s
psp-prometheus-prometheus-node-exporter  34s

==> v1beta1/ClusterRoleBinding
NAME                                     AGE
prometheus-kube-state-metrics            34s
psp-prometheus-prometheus-node-exporter  34s

==> v1beta1/DaemonSet
NAME                                 AGE
prometheus-prometheus-node-exporter  34s

==> v1beta1/MutatingWebhookConfiguration
NAME                                  AGE
prometheus-prometheus-oper-admission  34s

==> v1beta1/PodSecurityPolicy
NAME                                     AGE
prometheus-grafana                       34s
prometheus-grafana-test                  34s
prometheus-kube-state-metrics            34s
prometheus-prometheus-node-exporter      34s
prometheus-prometheus-oper-alertmanager  34s
prometheus-prometheus-oper-operator      34s
prometheus-prometheus-oper-prometheus    34s

==> v1beta1/Role
NAME                AGE
prometheus-grafana  34s

==> v1beta1/RoleBinding
NAME                AGE
prometheus-grafana  34s

==> v1beta1/ValidatingWebhookConfiguration
NAME                                  AGE
prometheus-prometheus-oper-admission  34s


NOTES:
The Prometheus Operator has been installed. Check its status by running:
  kubectl --namespace prometheus get pods -l \"release=prometheus\"

Visit https://github.com/coreos/prometheus-operator for instructions on how
to create & configure Alertmanager and Prometheus instances using the Operator.

NAME                                                   READY   STATUS    RESTARTS   AGE
prometheus-grafana-5c97446694-9bhc7                    2/2     Running   0          2m34s
prometheus-prometheus-node-exporter-9nbsp              1/1     Running   0          2m34s
prometheus-prometheus-node-exporter-cnxtz              1/1     Running   0          2m34s
prometheus-prometheus-oper-operator-6d59dcfb57-2crvb   2/2     Running   0          2m34s

Continue [Y/n]?: y

Function [monitoring_services]:
{
    \"apiVersion\": \"v1\",
    \"items\": [
        {
            \"apiVersion\": \"v1\",
            \"kind\": \"Service\",
            \"metadata\": {
                \"creationTimestamp\": \"2019-12-23T08:48:24Z\",
                \"labels\": {
                    \"operated-alertmanager\": \"true\"
                },
                \"name\": \"alertmanager-operated\",
                \"namespace\": \"prometheus\",
                \"ownerReferences\": [
                    {
                        \"apiVersion\": \"monitoring.coreos.com/v1\",
                        \"kind\": \"Alertmanager\",
                        \"name\": \"prometheus-prometheus-oper-alertmanager\",
                        \"uid\": \"f78a4893-2560-11ea-83e0-0a20636a20d3\"
                    }
                ],
                \"resourceVersion\": \"2013\",
                \"selfLink\": \"/api/v1/namespaces/prometheus/services/alertmanager-operated\",
                \"uid\": \"fb14a3bd-2560-11ea-a71a-02287af92a37\"
            },
            \"spec\": {
                \"clusterIP\": \"None\",
                \"ports\": [
                    {
                        \"name\": \"web\",
                        \"port\": 9093,
                        \"protocol\": \"TCP\",
                        \"targetPort\": 9093
                    },
                    {
                        \"name\": \"mesh-tcp\",
                        \"port\": 9094,
                        \"protocol\": \"TCP\",
                        \"targetPort\": 9094
                    },
                    {
                        \"name\": \"mesh-udp\",
                        \"port\": 9094,
                        \"protocol\": \"UDP\",
                        \"targetPort\": 9094
                    }
                ],
                \"selector\": {
                    \"app\": \"alertmanager\"
                },
                \"sessionAffinity\": \"None\",
                \"type\": \"ClusterIP\"
            },
            \"status\": {
                \"loadBalancer\": {}
            }
        },
        {
            \"apiVersion\": \"v1\",
            \"kind\": \"Service\",
            \"metadata\": {
                \"creationTimestamp\": \"2019-12-23T08:48:18Z\",
                \"labels\": {
                    \"app\": \"grafana\",
                    \"chart\": \"grafana-3.8.19\",
                    \"heritage\": \"Tiller\",
                    \"release\": \"prometheus\"
                },
                \"name\": \"prometheus-grafana\",
                \"namespace\": \"prometheus\",
                \"resourceVersion\": \"1866\",
                \"selfLink\": \"/api/v1/namespaces/prometheus/services/prometheus-grafana\",
                \"uid\": \"f783a5ef-2560-11ea-83e0-0a20636a20d3\"
            },
            \"spec\": {
                \"clusterIP\": \"10.100.200.160\",
                \"ports\": [
                    {
                        \"name\": \"service\",
                        \"port\": 80,
                        \"protocol\": \"TCP\",
                        \"targetPort\": 3000
                    }
                ],
                \"selector\": {
                    \"app\": \"grafana\",
                    \"release\": \"prometheus\"
                },
                \"sessionAffinity\": \"None\",
                \"type\": \"ClusterIP\"
            },
            \"status\": {
                \"loadBalancer\": {}
            }
        },
        {
            \"apiVersion\": \"v1\",
            \"kind\": \"Service\",
            \"metadata\": {
                \"annotations\": {
                    \"prometheus.io/scrape\": \"true\"
                },
                \"creationTimestamp\": \"2019-12-23T08:48:18Z\",
                \"labels\": {
                    \"app.kubernetes.io/instance\": \"prometheus\",
                    \"app.kubernetes.io/managed-by\": \"Tiller\",
                    \"app.kubernetes.io/name\": \"kube-state-metrics\",
                    \"helm.sh/chart\": \"kube-state-metrics-2.3.1\"
                },
                \"name\": \"prometheus-kube-state-metrics\",
                \"namespace\": \"prometheus\",
                \"resourceVersion\": \"1860\",
                \"selfLink\": \"/api/v1/namespaces/prometheus/services/prometheus-kube-state-metrics\",
                \"uid\": \"f781b42d-2560-11ea-83e0-0a20636a20d3\"
            },
            \"spec\": {
                \"clusterIP\": \"10.100.234.122\",
                \"ports\": [
                    {
                        \"name\": \"http\",
                        \"port\": 8080,
                        \"protocol\": \"TCP\",
                        \"targetPort\": 8080
                    }
                ],
                \"selector\": {
                    \"app.kubernetes.io/instance\": \"prometheus\",
                    \"app.kubernetes.io/name\": \"kube-state-metrics\"
                },
                \"sessionAffinity\": \"None\",
                \"type\": \"ClusterIP\"
            },
            \"status\": {
                \"loadBalancer\": {}
            }
        },
        {
            \"apiVersion\": \"v1\",
            \"kind\": \"Service\",
            \"metadata\": {
                \"creationTimestamp\": \"2019-12-23T08:48:34Z\",
                \"labels\": {
                    \"operated-prometheus\": \"true\"
                },
                \"name\": \"prometheus-operated\",
                \"namespace\": \"prometheus\",
                \"ownerReferences\": [
                    {
                        \"apiVersion\": \"monitoring.coreos.com/v1\",
                        \"kind\": \"Prometheus\",
                        \"name\": \"prometheus-prometheus-oper-prometheus\",
                        \"uid\": \"f7983943-2560-11ea-83e0-0a20636a20d3\"
                    }
                ],
                \"resourceVersion\": \"2095\",
                \"selfLink\": \"/api/v1/namespaces/prometheus/services/prometheus-operated\",
                \"uid\": \"00f067cb-2561-11ea-a71a-02287af92a37\"
            },
            \"spec\": {
                \"clusterIP\": \"None\",
                \"ports\": [
                    {
                        \"name\": \"web\",
                        \"port\": 9090,
                        \"protocol\": \"TCP\",
                        \"targetPort\": \"web\"
                    }
                ],
                \"selector\": {
                    \"app\": \"prometheus\"
                },
                \"sessionAffinity\": \"None\",
                \"type\": \"ClusterIP\"
            },
            \"status\": {
                \"loadBalancer\": {}
            }
        },
        {
            \"apiVersion\": \"v1\",
            \"kind\": \"Service\",
            \"metadata\": {
                \"annotations\": {
                    \"prometheus.io/scrape\": \"true\"
                },
                \"creationTimestamp\": \"2019-12-23T08:48:18Z\",
                \"labels\": {
                    \"app\": \"prometheus-node-exporter\",
                    \"chart\": \"prometheus-node-exporter-1.5.2\",
                    \"heritage\": \"Tiller\",
                    \"jobLabel\": \"node-exporter\",
                    \"release\": \"prometheus\"
                },
                \"name\": \"prometheus-prometheus-node-exporter\",
                \"namespace\": \"prometheus\",
                \"resourceVersion\": \"1854\",
                \"selfLink\": \"/api/v1/namespaces/prometheus/services/prometheus-prometheus-node-exporter\",
                \"uid\": \"f78066ae-2560-11ea-83e0-0a20636a20d3\"
            },
            \"spec\": {
                \"clusterIP\": \"10.100.189.57\",
                \"ports\": [
                    {
                        \"name\": \"metrics\",
                        \"port\": 9100,
                        \"protocol\": \"TCP\",
                        \"targetPort\": 9100
                    }
                ],
                \"selector\": {
                    \"app\": \"prometheus-node-exporter\",
                    \"release\": \"prometheus\"
                },
                \"sessionAffinity\": \"None\",
                \"type\": \"ClusterIP\"
            },
            \"status\": {
                \"loadBalancer\": {}
            }
        },
        {
            \"apiVersion\": \"v1\",
            \"kind\": \"Service\",
            \"metadata\": {
                \"creationTimestamp\": \"2019-12-23T08:48:18Z\",
                \"labels\": {
                    \"app\": \"prometheus-operator-alertmanager\",
                    \"chart\": \"prometheus-operator-6.18.0\",
                    \"heritage\": \"Tiller\",
                    \"release\": \"prometheus\"
                },
                \"name\": \"prometheus-prometheus-oper-alertmanager\",
                \"namespace\": \"prometheus\",
                \"resourceVersion\": \"1862\",
                \"selfLink\": \"/api/v1/namespaces/prometheus/services/prometheus-prometheus-oper-alertmanager\",
                \"uid\": \"f782b095-2560-11ea-83e0-0a20636a20d3\"
            },
            \"spec\": {
                \"clusterIP\": \"10.100.223.31\",
                \"ports\": [
                    {
                        \"name\": \"web\",
                        \"port\": 9093,
                        \"protocol\": \"TCP\",
                        \"targetPort\": 9093
                    }
                ],
                \"selector\": {
                    \"alertmanager\": \"prometheus-prometheus-oper-alertmanager\",
                    \"app\": \"alertmanager\"
                },
                \"sessionAffinity\": \"None\",
                \"type\": \"ClusterIP\"
            },
            \"status\": {
                \"loadBalancer\": {}
            }
        },
        {
            \"apiVersion\": \"v1\",
            \"kind\": \"Service\",
            \"metadata\": {
                \"creationTimestamp\": \"2019-12-23T08:48:18Z\",
                \"labels\": {
                    \"app\": \"prometheus-operator-operator\",
                    \"chart\": \"prometheus-operator-6.18.0\",
                    \"heritage\": \"Tiller\",
                    \"release\": \"prometheus\"
                },
                \"name\": \"prometheus-prometheus-oper-operator\",
                \"namespace\": \"prometheus\",
                \"resourceVersion\": \"1869\",
                \"selfLink\": \"/api/v1/namespaces/prometheus/services/prometheus-prometheus-oper-operator\",
                \"uid\": \"f784a6ea-2560-11ea-83e0-0a20636a20d3\"
            },
            \"spec\": {
                \"clusterIP\": \"10.100.234.247\",
                \"ports\": [
                    {
                        \"name\": \"http\",
                        \"port\": 8080,
                        \"protocol\": \"TCP\",
                        \"targetPort\": \"http\"
                    },
                    {
                        \"name\": \"https\",
                        \"port\": 443,
                        \"protocol\": \"TCP\",
                        \"targetPort\": \"https\"
                    }
                ],
                \"selector\": {
                    \"app\": \"prometheus-operator-operator\",
                    \"release\": \"prometheus\"
                },
                \"sessionAffinity\": \"None\",
                \"type\": \"ClusterIP\"
            },
            \"status\": {
                \"loadBalancer\": {}
            }
        },
        {
            \"apiVersion\": \"v1\",
            \"kind\": \"Service\",
            \"metadata\": {
                \"creationTimestamp\": \"2019-12-23T08:48:18Z\",
                \"labels\": {
                    \"app\": \"prometheus-operator-prometheus\",
                    \"chart\": \"prometheus-operator-6.18.0\",
                    \"heritage\": \"Tiller\",
                    \"release\": \"prometheus\"
                },
                \"name\": \"prometheus-prometheus-oper-prometheus\",
                \"namespace\": \"prometheus\",
                \"resourceVersion\": \"1850\",
                \"selfLink\": \"/api/v1/namespaces/prometheus/services/prometheus-prometheus-oper-prometheus\",
                \"uid\": \"f77e1ac6-2560-11ea-83e0-0a20636a20d3\"
            },
            \"spec\": {
                \"clusterIP\": \"10.100.1.93\",
                \"ports\": [
                    {
                        \"name\": \"web\",
                        \"port\": 9090,
                        \"protocol\": \"TCP\",
                        \"targetPort\": 9090
                    }
                ],
                \"selector\": {
                    \"app\": \"prometheus\",
                    \"prometheus\": \"prometheus-prometheus-oper-prometheus\"
                },
                \"sessionAffinity\": \"None\",
                \"type\": \"ClusterIP\"
            },
            \"status\": {
                \"loadBalancer\": {}
            }
        }
    ],
    \"kind\": \"List\",
    \"metadata\": {
        \"resourceVersion\": \"\",
        \"selfLink\": \"\"
    }
}

Continue [Y/n]?: y

Function [display_pods]: prometheus
NAME                                                     READY   STATUS    RESTARTS   AGE     IP               NODE                             NOMINATED NODE   READINESS GATES
prometheus-grafana-5c97446694-9bhc7                      2/2     Running   0          2m35s   192.168.37.136   ip-192-168-52-90.ec2.internal    <none>           <none>
prometheus-kube-state-metrics-5ffdf76ddd-f8pgx           1/1     Running   0          2m35s   192.168.38.138   ip-192-168-52-90.ec2.internal    <none>           <none>
prometheus-prometheus-node-exporter-9nbsp                1/1     Running   0          2m35s   192.168.52.90    ip-192-168-52-90.ec2.internal    <none>           <none>
alertmanager-prometheus-prometheus-oper-alertmanager-0   2/2     Running   0          2m29s   192.168.71.52    ip-192-168-66-209.ec2.internal   <none>           <none>
prometheus-prometheus-node-exporter-cnxtz                1/1     Running   0          2m35s   192.168.66.209   ip-192-168-66-209.ec2.internal   <none>           <none>
prometheus-prometheus-oper-operator-6d59dcfb57-2crvb     2/2     Running   0          2m35s   192.168.89.83    ip-192-168-66-209.ec2.internal   <none>           <none>
prometheus-prometheus-prometheus-oper-prometheus-0       3/3     Running   1          2m19s   192.168.64.76    ip-192-168-66-209.ec2.internal   <none>           <none>

Continue [Y/n]?: y

Function [deploy_dashboard]:
Creating Kubernetes Dashboard ...

secret/kubernetes-dashboard-certs created
serviceaccount/kubernetes-dashboard created
role.rbac.authorization.k8s.io/kubernetes-dashboard-minimal created
rolebinding.rbac.authorization.k8s.io/kubernetes-dashboard-minimal created
deployment.apps/kubernetes-dashboard created
service/kubernetes-dashboard created

Generating console-user and credentials ...

serviceaccount/admin-user created
clusterrolebinding.rbac.authorization.k8s.io/admin-user created

Displaying console-user configuration ...

Name:         admin-user-token-xn5nz
Namespace:    kube-system
Labels:       <none>
Annotations:  kubernetes.io/service-account.name: admin-user
              kubernetes.io/service-account.uid: 556e810c-2561-11ea-a71a-02287af92a37

Type:  kubernetes.io/service-account-token

Data
====
ca.crt:     1025 bytes
namespace:  11 bytes
token:      eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLXhuNW56Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiI1NTZlODEwYy0yNTYxLTExZWEtYTcxYS0wMjI4N2FmOTJhMzciLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06YWRtaW4tdXNlciJ9.hPd7hlaTloIxTDGceoTYCKhvgke1mw7bNxVmuUey-kCX45-tSUlYjnHiXsbO7JY9hnRJBz5uMZBTxOgqy7WqJB4zkFygscywn1KvTHa7JzYEfSVDd7ND7UrKp79vFpjz8M4eMPsKEiNkbvBwPqSAVOmAoEmMpPC3PlSa17t1rf9W09CrmoT86va8vtnBac47dz9vQsUNUlrBH_Jc5gFt6eiUqkJKZzhMWhTqIIOXbCu9nreRIjrZOIHr1GL_gpRGstRzMBVwkmj4kzYJI0vfZPjhYuXqGru4GonleatOK5PBEB16gWKS5BEr-IOncfuKea1yew7qhJ97c8qAqoa7GQ

Execute: kubectl proxy ;
http://127.0.0.1:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/#!/login

Console User-Token:

eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLXhuNW56Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiI1NTZlODEwYy0yNTYxLTExZWEtYTcxYS0wMjI4N2FmOTJhMzciLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06YWRtaW4tdXNlciJ9.hPd7hlaTloIxTDGceoTYCKhvgke1mw7bNxVmuUey-kCX45-tSUlYjnHiXsbO7JY9hnRJBz5uMZBTxOgqy7WqJB4zkFygscywn1KvTHa7JzYEfSVDd7ND7UrKp79vFpjz8M4eMPsKEiNkbvBwPqSAVOmAoEmMpPC3PlSa17t1rf9W09CrmoT86va8vtnBac47dz9vQsUNUlrBH_Jc5gFt6eiUqkJKZzhMWhTqIIOXbCu9nreRIjrZOIHr1GL_gpRGstRzMBVwkmj4kzYJI0vfZPjhYuXqGru4GonleatOK5PBEB16gWKS5BEr-IOncfuKea1yew7qhJ97c8qAqoa7GQ

Continue [Y/n]?: y

Deploy Prototype-Application (Y/n) ?: y

Function [cluster_application]:
Generating Application Container (Pod) ...

deployment.apps/helloworld-deployment created

Exposing Deployment Object (Load-Balancer) ...

service/helloworld-deployment exposed

Identifying Deployment Status ...

Waiting for deployment \"helloworld-deployment\" rollout to finish: 0 of 3 updated replicas are available...
Waiting for deployment \"helloworld-deployment\" rollout to finish: 1 of 3 updated replicas are available...
Waiting for deployment \"helloworld-deployment\" rollout to finish: 2 of 3 updated replicas are available...
deployment \"helloworld-deployment\" successfully rolled out

Continue [Y/n]?: y

Function [cluster_services]:
Listing Kubernetes Services:

NAME                    TYPE           CLUSTER-IP      EXTERNAL-IP                                                               PORT(S)          AGE
helloworld-deployment   LoadBalancer   10.100.162.98   a5740d528256111eaa71a02287af92a3-1480566631.us-east-1.elb.amazonaws.com   3000:30583/TCP   19s
kubernetes              ClusterIP      10.100.0.1      <none>                                                                    443/TCP          12m

Describing Application Service ...

Name:                     helloworld-deployment
Namespace:                default
Labels:                   <none>
Annotations:              <none>
Selector:                 app=helloworld
Type:                     LoadBalancer
IP:                       10.100.162.98
LoadBalancer Ingress:     a5740d528256111eaa71a02287af92a3-1480566631.us-east-1.elb.amazonaws.com
Port:                     <unset>  3000/TCP
TargetPort:               3000/TCP
NodePort:                 <unset>  30583/TCP
Endpoints:                192.168.33.101:3000,192.168.60.116:3000,192.168.87.199:3000
Session Affinity:         None
External Traffic Policy:  Cluster
Events:
  Type    Reason                Age   From                Message
  ----    ------                ----  ----                -------
  Normal  EnsuringLoadBalancer  20s   service-controller  Ensuring load balancer
  Normal  EnsuredLoadBalancer   18s   service-controller  Ensured load balancer


Name:              kubernetes
Namespace:         default
Labels:            component=apiserver
                   provider=kubernetes
Annotations:       <none>
Selector:          <none>
Type:              ClusterIP
IP:                10.100.0.1
Port:              https  443/TCP
TargetPort:        443/TCP
Endpoints:         192.168.113.174:443,192.168.178.220:443
Session Affinity:  None
Events:            <none>

Continue [Y/n]?: y

Function [cluster_replicationsets]:
Listing Replication Sets:

NAME                               DESIRED   CURRENT   READY   AGE
helloworld-deployment-79789c9f5d   3         3         3       21s

Continue [Y/n]?: y

Function [cluster_deployments]: default
Listing Deployment Objects [default]:

NAME                    READY   UP-TO-DATE   AVAILABLE   AGE
helloworld-deployment   3/3     3            3           22s

Continue [Y/n]?: y

Function [extract_deployment]: helloworld-deployment
Cluster Deployment: helloworld-deployment-79789c9f5d-bpn87

Continue [Y/n]?: y

Describing Deployment: helloworld-deployment-79789c9f5d-bpn87

Function [describe_podname]: helloworld-deployment-79789c9f5d-bpn87
Name:           helloworld-deployment-79789c9f5d-bpn87
Namespace:      default
Priority:       0
Node:           ip-192-168-52-90.ec2.internal/192.168.52.90
Start Time:     Mon, 23 Dec 2019 01:50:58 -0700
Labels:         app=helloworld
                pod-template-hash=79789c9f5d
Annotations:    kubernetes.io/psp: eks.privileged
Status:         Running
IP:             192.168.33.101
IPs:            <none>
Controlled By:  ReplicaSet/helloworld-deployment-79789c9f5d
Containers:
  k8s-demo:
    Container ID:   docker://832d709eefb1665e9108d587cdecb41d2a52205004298ecacaf50e78adf58b08
    Image:          wardviaene/k8s-demo
    Image ID:       docker-pullable://wardviaene/k8s-demo@sha256:2c050f462f5d0b3a6430e7869bcdfe6ac48a447a89da79a56d0ef61460c7ab9e
    Port:           3000/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Mon, 23 Dec 2019 01:51:14 -0700
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-g7zwr (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             True
  ContainersReady   True
  PodScheduled      True
Volumes:
  default-token-g7zwr:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-g7zwr
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type    Reason     Age   From                                    Message
  ----    ------     ----  ----                                    -------
  Normal  Scheduled  23s   default-scheduler                       Successfully assigned default/helloworld-deployment-79789c9f5d-bpn87 to ip-192-168-52-90.ec2.internal
  Normal  Pulling    21s   kubelet, ip-192-168-52-90.ec2.internal  Pulling image \"wardviaene/k8s-demo\"
  Normal  Pulled     10s   kubelet, ip-192-168-52-90.ec2.internal  Successfully pulled image \"wardviaene/k8s-demo\"
  Normal  Created    7s    kubelet, ip-192-168-52-90.ec2.internal  Created container k8s-demo
  Normal  Started    7s    kubelet, ip-192-168-52-90.ec2.internal  Started container k8s-demo

Continue [Y/n]?: y

Displaying Deployment's Log: helloworld-deployment-79789c9f5d-bpn87

npm info it worked if it ends with ok
npm info using npm@2.15.11
npm info using node@v4.6.2
npm info prestart myapp@0.0.1
npm info start myapp@0.0.1

> myapp@0.0.1 start /app
> node index.js

Example app listening at http://:::3000

Continue [Y/n]?: y

Describe Kubernetes Cluster (Y/n) ?: y

Function [cluster_configuration]:

Listing Infrastructure Clusters ...

NAME
prototype.us-east-1.eksctl.io

Listing Kubernetes Configuration ...

apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: DATA+OMITTED
    server: https://31D0097B30C6300A1AD47CD740DB4E4E.yl4.us-east-1.eks.amazonaws.com
  name: prototype.us-east-1.eksctl.io
contexts:
- context:
    cluster: prototype.us-east-1.eksctl.io
    user: kubernetes@prototype.us-east-1.eksctl.io
  name: kubernetes@prototype.us-east-1.eksctl.io
current-context: kubernetes@prototype.us-east-1.eksctl.io
kind: Config
preferences: {}
users:
- name: kubernetes@prototype.us-east-1.eksctl.io
  user:
    exec:
      apiVersion: client.authentication.k8s.io/v1alpha1
      args:
      - token
      - -i
      - prototype
      command: aws-iam-authenticator
      env:
      - name: AWS_PROFILE
        value: kubernetes

Continue [Y/n]?: y

Function [cluster_validation]:

Validating Kubernetes Cluster:  ...

Using cluster from kubectl context: prototype.us-east-1.eksctl.io


State Store: Required value: Please set the --state flag or export KOPS_STATE_STORE.
For example, a valid value follows the format s3://<bucket>.
You can find the supported stores in https://github.com/kubernetes/kops/blob/master/docs/state.md.

Continue [Y/n]?: y

Function [cluster_deployments]: kube-system
Listing Deployment Objects [kube-system]:

NAME                   READY   UP-TO-DATE   AVAILABLE   AGE
cluster-autoscaler     1/1     1            1           6m46s
cni-metrics-helper     1/1     1            1           5m39s
coredns                2/2     2            2           12m
kubernetes-dashboard   1/1     1            1           42s
metrics-server         1/1     1            1           6m51s
tiller-deploy          1/1     1            1           5m2s

Continue [Y/n]?: y

Function [display_apiservice]:

NAME                                   SERVICE                      AVAILABLE   AGE
v1.                                    Local                        True        12m
v1.apps                                Local                        True        12m
v1.authentication.k8s.io               Local                        True        12m
v1.authorization.k8s.io                Local                        True        12m
v1.autoscaling                         Local                        True        12m
v1.batch                               Local                        True        12m
v1.coordination.k8s.io                 Local                        True        12m
v1.monitoring.coreos.com               Local                        True        3m58s
v1.networking.k8s.io                   Local                        True        12m
v1.rbac.authorization.k8s.io           Local                        True        12m
v1.scheduling.k8s.io                   Local                        True        12m
v1.storage.k8s.io                      Local                        True        12m
v1alpha1.crd.k8s.amazonaws.com         Local                        True        9m32s
v1beta1.admissionregistration.k8s.io   Local                        True        12m
v1beta1.apiextensions.k8s.io           Local                        True        12m
v1beta1.apps                           Local                        True        12m
v1beta1.authentication.k8s.io          Local                        True        12m
v1beta1.authorization.k8s.io           Local                        True        12m
v1beta1.batch                          Local                        True        12m
v1beta1.certificates.k8s.io            Local                        True        12m
v1beta1.coordination.k8s.io            Local                        True        12m
v1beta1.events.k8s.io                  Local                        True        12m
v1beta1.extensions                     Local                        True        12m
v1beta1.metrics.k8s.io                 kube-system/metrics-server   True        6m52s
v1beta1.networking.k8s.io              Local                        True        12m
v1beta1.node.k8s.io                    Local                        True        12m
v1beta1.policy                         Local                        True        12m
v1beta1.rbac.authorization.k8s.io      Local                        True        12m
v1beta1.scheduling.k8s.io              Local                        True        12m
v1beta1.storage.k8s.io                 Local                        True        12m
v1beta2.apps                           Local                        True        12m
v2beta1.autoscaling                    Local                        True        12m
v2beta2.autoscaling                    Local                        True        12m

Continue [Y/n]?: y

Function [display_namespaces]:

NAME                                    CREATED AT
alertmanagers.monitoring.coreos.com     2019-12-23T08:47:40Z
eniconfigs.crd.k8s.amazonaws.com        2019-12-23T08:39:15Z
podmonitors.monitoring.coreos.com       2019-12-23T08:47:41Z
prometheuses.monitoring.coreos.com      2019-12-23T08:47:42Z
prometheusrules.monitoring.coreos.com   2019-12-23T08:47:43Z
servicemonitors.monitoring.coreos.com   2019-12-23T08:47:44Z

Continue [Y/n]?: y

Function [display_nodes]:

NAME                             STATUS   ROLES    AGE     VERSION              INTERNAL-IP      EXTERNAL-IP    OS-IMAGE         KERNEL-VERSION                  CONTAINER-RUNTIME
ip-192-168-52-90.ec2.internal    Ready    <none>   8m36s   v1.14.7-eks-1861c5   192.168.52.90    3.82.219.54    Amazon Linux 2   4.14.146-119.123.amzn2.x86_64   docker://18.6.1
ip-192-168-66-209.ec2.internal   Ready    <none>   8m51s   v1.14.7-eks-1861c5   192.168.66.209   3.87.238.174   Amazon Linux 2   4.14.146-119.123.amzn2.x86_64   docker://18.6.1

Continue [Y/n]?: y

Function [display_labels]:

Listing Kubernetes Nodes:

NAME                             STATUS   ROLES    AGE     VERSION              LABELS
ip-192-168-52-90.ec2.internal    Ready    <none>   8m37s   v1.14.7-eks-1861c5   alpha.eksctl.io/cluster-name=prototype,alpha.eksctl.io/nodegroup-name=devops,beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=m5.large,beta.kubernetes.io/os=linux,eks.amazonaws.com/nodegroup-image=ami-0392bafc801b7520f,eks.amazonaws.com/nodegroup=devops,failure-domain.beta.kubernetes.io/region=us-east-1,failure-domain.beta.kubernetes.io/zone=us-east-1b,kubernetes.io/arch=amd64,kubernetes.io/hostname=ip-192-168-52-90.ec2.internal,kubernetes.io/os=linux
ip-192-168-66-209.ec2.internal   Ready    <none>   8m52s   v1.14.7-eks-1861c5   alpha.eksctl.io/cluster-name=prototype,alpha.eksctl.io/nodegroup-name=devops,beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=m5.large,beta.kubernetes.io/os=linux,eks.amazonaws.com/nodegroup-image=ami-0392bafc801b7520f,eks.amazonaws.com/nodegroup=devops,failure-domain.beta.kubernetes.io/region=us-east-1,failure-domain.beta.kubernetes.io/zone=us-east-1c,kubernetes.io/arch=amd64,kubernetes.io/hostname=ip-192-168-66-209.ec2.internal,kubernetes.io/os=linux

Continue [Y/n]?: y

Function [display_pods]: default
NAME                                     READY   STATUS    RESTARTS   AGE   IP               NODE                             NOMINATED NODE   READINESS GATES
helloworld-deployment-79789c9f5d-bpn87   1/1     Running   0          41s   192.168.33.101   ip-192-168-52-90.ec2.internal    <none>           <none>
helloworld-deployment-79789c9f5d-t88mw   1/1     Running   0          41s   192.168.60.116   ip-192-168-52-90.ec2.internal    <none>           <none>
helloworld-deployment-79789c9f5d-k4lmd   1/1     Running   0          41s   192.168.87.199   ip-192-168-66-209.ec2.internal   <none>           <none>

Continue [Y/n]?: y

Function [display_pods]: kube-system
NAME                                    READY   STATUS    RESTARTS   AGE     IP               NODE                             NOMINATED NODE   READINESS GATES
aws-node-t9nlt                          1/1     Running   0          8m38s   192.168.52.90    ip-192-168-52-90.ec2.internal    <none>           <none>
kubernetes-dashboard-5f7b999d65-jjvds   1/1     Running   0          45s     192.168.53.171   ip-192-168-52-90.ec2.internal    <none>           <none>
cni-metrics-helper-7b845dfdd4-vvw9k     1/1     Running   0          5m42s   192.168.52.3     ip-192-168-52-90.ec2.internal    <none>           <none>
coredns-56678dcf76-shwk5                1/1     Running   0          12m     192.168.39.216   ip-192-168-52-90.ec2.internal    <none>           <none>
kube-proxy-glnt4                        1/1     Running   0          8m38s   192.168.52.90    ip-192-168-52-90.ec2.internal    <none>           <none>
metrics-server-7fcf9cc98b-5fk9s         1/1     Running   0          6m54s   192.168.41.92    ip-192-168-52-90.ec2.internal    <none>           <none>
aws-node-nv7hm                          1/1     Running   0          8m53s   192.168.66.209   ip-192-168-66-209.ec2.internal   <none>           <none>
coredns-56678dcf76-5wk8r                1/1     Running   0          12m     192.168.65.72    ip-192-168-66-209.ec2.internal   <none>           <none>
kube-proxy-7bzxn                        1/1     Running   0          8m53s   192.168.66.209   ip-192-168-66-209.ec2.internal   <none>           <none>
cluster-autoscaler-85c6b59c96-wj7qj     1/1     Running   0          5m58s   192.168.86.113   ip-192-168-66-209.ec2.internal   <none>           <none>
tiller-deploy-54c98f988f-srz4r          1/1     Running   0          5m5s    192.168.82.224   ip-192-168-66-209.ec2.internal   <none>           <none>

Continue [Y/n]?: y

Function [display_pods]: prometheus
NAME                                                     READY   STATUS    RESTARTS   AGE     IP               NODE                             NOMINATED NODE   READINESS GATES
prometheus-grafana-5c97446694-9bhc7                      2/2     Running   0          3m22s   192.168.37.136   ip-192-168-52-90.ec2.internal    <none>           <none>
prometheus-kube-state-metrics-5ffdf76ddd-f8pgx           1/1     Running   0          3m22s   192.168.38.138   ip-192-168-52-90.ec2.internal    <none>           <none>
prometheus-prometheus-node-exporter-9nbsp                1/1     Running   0          3m22s   192.168.52.90    ip-192-168-52-90.ec2.internal    <none>           <none>
alertmanager-prometheus-prometheus-oper-alertmanager-0   2/2     Running   0          3m16s   192.168.71.52    ip-192-168-66-209.ec2.internal   <none>           <none>
prometheus-prometheus-node-exporter-cnxtz                1/1     Running   0          3m22s   192.168.66.209   ip-192-168-66-209.ec2.internal   <none>           <none>
prometheus-prometheus-oper-operator-6d59dcfb57-2crvb     2/2     Running   0          3m22s   192.168.89.83    ip-192-168-66-209.ec2.internal   <none>           <none>
prometheus-prometheus-prometheus-oper-prometheus-0       3/3     Running   1          3m6s    192.168.64.76    ip-192-168-66-209.ec2.internal   <none>           <none>

Continue [Y/n]?: y

Kubernetes master is running at https://31D0097B30C6300A1AD47CD740DB4E4E.yl4.us-east-1.eks.amazonaws.com
CoreDNS is running at https://31D0097B30C6300A1AD47CD740DB4E4E.yl4.us-east-1.eks.amazonaws.com/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
Metrics-server is running at https://31D0097B30C6300A1AD47CD740DB4E4E.yl4.us-east-1.eks.amazonaws.com/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.

Function [display_ekscluster]:
[
    {
        \"Arn\": \"arn:aws:eks:us-east-1:738054984624:cluster/prototype\",
        \"CertificateAuthority\": {
            \"Data\": \"LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRFNU1USXlNekE0TXpneU9Wb1hEVEk1TVRJeU1EQTRNemd5T1Zvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBS21RCmJFRmZkTEdTSUU5YTgzY3ZIMFBqblJOZEo3aHN2VDV2UlpLcFlOZlg3TWdiZ0hVOG11WXdJYnZVQjJDSXZNK3kKN2M2QVozeUsrSTd5WjIxck5kSlBsaFV5RThZd09RdGJxOGNDdHl2RklFUnRxRXVrbmo2MW1ET01LTXFjeDdpQgpySTdHWVFrbmtYME83VUdhQWRpOExIaXBCTXdpTHI5bEhHQXhQSGx6V3pJcTdEUXNBc0VnaW9CTHhNVUtOaFRwCmljeFFIOVU2MXdtZGFIeXJ3WFRubm4vWGRBdUtmWVRmOWZzczlXa2ZDTTdCdmZzNjUxaUh5WVFsaDJJNjJCcTAKWWFDNUt1ODFkam4rZXVZak1TTFg1QlJrZTdDMFBqeHgzT0huek52R2lzVGNxcHNjZktuWVRRWStpT0IzNDh6SwpKYzRnYkNNWFl1djhUalB1bTVjQ0F3RUFBYU1qTUNFd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFIZ3RsSk9zTWkraWZST1RMWERrLzhwbksyOHIKNGdvWFpCTXNoaVpKYXhWMWo1WURTSUc5dzZlNUUyZlQ3d05Ud0ZLWWk2MXNodERIK1NLaGRRaFBtN1FaLy9EOAphaGRORnNpTkdYVHhMVzhPSlNXa1graWZXWStKS3AzWWVlWkN1bDFYd3lkc0xsMGJSYldaNVBEdk9VQW1jNng3CmcyZEMyekxBaklRdU1KbHdjWGFEYmF6UzRra1VSVkpWZFNpUW5tOW5nZmttWDZrSmtJeDJENmZjV0FSeUFZVGgKVGJsWFFoMGY5L2YweDdIUHNvOFBWeDJXV1JRbnZoanlnVVlOK3BUNEduTm9DZkliVHBVS2pOQjJsMkpjNElZawpUVmhKSHlSaVNzSWZLL1J5T0F0RWxvckRqU0N4YXFmV2tVU0pYZXFjM2IvVkJIa1RVeVhvRElwTEVYTT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=\"
        },
        \"ClientRequestToken\": null,
        \"CreatedAt\": \"2019-12-23T08:29:20Z\",
        \"Endpoint\": \"https://31D0097B30C6300A1AD47CD740DB4E4E.yl4.us-east-1.eks.amazonaws.com\",
        \"Identity\": {
            \"Oidc\": {
                \"Issuer\": \"https://oidc.eks.us-east-1.amazonaws.com/id/31D0097B30C6300A1AD47CD740DB4E4E\"
            }
        },
        \"Logging\": {
            \"ClusterLogging\": [
                {
                    \"Enabled\": true,
                    \"Types\": [
                        \"api\",
                        \"audit\",
                        \"authenticator\",
                        \"controllerManager\",
                        \"scheduler\"
                    ]
                }
            ]
        },
        \"Name\": \"prototype\",
        \"PlatformVersion\": \"eks.6\",
        \"ResourcesVpcConfig\": {
            \"ClusterSecurityGroupId\": \"sg-0802d870efe4f66a2\",
            \"EndpointPrivateAccess\": false,
            \"EndpointPublicAccess\": true,
            \"SecurityGroupIds\": [
                \"sg-054594ae12482ff56\"
            ],
            \"SubnetIds\": [
                \"subnet-0f5f396fc169fa3cc\",
                \"subnet-09a7c44bb0d763ea1\",
                \"subnet-0f2ee3e2b65f9660f\",
                \"subnet-06f80d112a35bcbc5\",
                \"subnet-088cf0f3c3ca8d8a3\",
                \"subnet-03804f05568eb39ba\"
            ],
            \"VpcId\": \"vpc-094eb6f95cb478207\"
        },
        \"RoleArn\": \"arn:aws:iam::738054984624:role/eksctl-prototype-cluster-ServiceRole-1NTUNUAY288AO\",
        \"Status\": \"ACTIVE\",
        \"Tags\": {},
        \"Version\": \"1.14\"
    }
]


Continue [Y/n]?: y

Completed!
";
    display_message "Goodbye!";
    exit 0;
    ## return 0;
  };
